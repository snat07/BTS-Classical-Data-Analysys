{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook we will present the polynomial regression through a practical exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of linear regression\n",
    "\n",
    "Linear regression requires the relation between the dependent variable and the independent variable to be linear. What if the distribution of the data was more complex as shown in the below figure? Can linear models be used to fit non-linear data? \n",
    "\n",
    "<img src=\"image_non-linear.png\">\n",
    "\n",
    "How can we generate a curve that best captures this kind of data?\n",
    "\n",
    "To understand the need for polynomial regression, let’s generate some random dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "x = 2 - 3 * np.random.normal(0, 1, 20)\n",
    "y = x - 2 * (x ** 2) + 0.5 * (x ** 3) + np.random.normal(-3, 3, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a scatter plot to visualize the generated points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should apply a linear model to this data, use the ```LinearRegression``` method from ```sklearn``` and plot the result. Remember that in order to use the ```.fit()``` method you need to add a new axis to x and y with ```numpy```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the linear model really capture the variability of the data? This is called *under-fitting*. This term is used when the model used to fit the data is too simple to really capture all the complexity of the data. To have a more quantitative measure of this under-fitting, compute the $R²$ and Root Mean Squared Error of the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pretty straight-forward way to add complexity to this model, is to add powers of the original features as new features. In this manner, the lineal model:\n",
    "\n",
    "$$y=\\theta_0+ \\theta_1x$$ \n",
    "\n",
    "derives to,\n",
    "\n",
    "$$y=\\theta_0+ \\theta_1x + \\theta_2x²$$ \n",
    "\n",
    "Now the beauty of this trick, is that you are still training a linear model. Which means you can still use the same algorithms to train it, but the curve that this will generate is actually quadratic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the original features into their higher order terms we will use the ```PolynomialFeatures``` class provided by ```scikit-learn```. And then we will use the same method to fit the curve and print the $R²$ and RMSE values, how do they compare to the previous case?. Do not plot the results just yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to plot the results this time we add this code to sort the x axis so that the plot is comparable (just execute the cell once you have your ```y_pred```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to see what happens if the we fit a polynomial of degree 3. What are the $R²$, and the RMSE? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit a model of degree 20, how do RMSE and $R²$ behave. Create a plot with the three fits overlaped. Do you think this is the model that better explains the data? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For degree=20, the model is also capturing the noise in the data. This is an example of *over-fitting*. Even though this model passes through most of the data, it will fail to generalize on unseen data.\n",
    "\n",
    "To prevent *over-fitting*, we can add more training samples so that the algorithm doesn’t learn the noise in the system and can become more generalized.\n",
    "How do we choose an optimal model? To answer this question we need to understand the bias vs variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance vs Bias trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Bias* refers to the error due to the model’s simplistic assumptions in fitting the data. A high bias means that the model is unable to capture the patterns in the data and this results in under-fitting.\n",
    "\n",
    "*Variance* refers to the error due to the complex model trying to fit the data. High variance means the model passes through most of the data points and it results in over-fitting the data.\n",
    "\n",
    "The below picture summarizes our learning.\n",
    "\n",
    "<img src=\"model_comparison.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the below picture we can observe that as the model complexity increases, the bias decreases and the variance increases and vice-versa. Ideally, a machine learning model should have low variance and low bias. But practically it’s impossible to have both. Therefore to achieve a good model that performs well both on the train and unseen data, a trade-off is made. Source: http://scott.fortmann-roe.com/docs/BiasVariance.html\n",
    "\n",
    "<img src=\"bias-variance.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about the behaviour of our metrics to evaluate the performance of the algorithm. Which ways could we use to control if our model is actually performing better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply polynomial regression to the Boston Housing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Boston Housing Dataset, and prepare train and test splits ```X_train, Y_train, X_test, Y_test```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function which will transform the original features into polynomial features of a given degree and then apply Linear Regression on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the function to a polynomial regression of degree 2. How does the result compare to our previous exercise? Would it make sense to use a higher degree?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
