{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge and Lasso regression are some of the simple techniques to reduce model complexity and prevent over-fitting which may result from simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression:\n",
    "\n",
    "In ridge regression, the cost function is altered by adding a penalty equivalent to square of the magnitude of the coefficients. This is also known as L2 regression\n",
    "\n",
    "<img src=\"Ridge_formula.png\">\n",
    "\n",
    "Now we will apply a ridge regression to the Boston Housing Data Set and compare the results with the linear regression performed in the previous Session. First load the pandas data set, and convert it to a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 12})\n",
    "from sklearn.datasets import load_boston\n",
    "boston_dataset = load_boston()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df = pd.DataFrame(data=boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "boston_df['MEDV'] = boston_dataset.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the X and y and split them randomly with sklearn, use 30% of the data for test and select the ```random_state=3```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define a linear regression model, and fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a ridge regression model, set ```alpha=0.3```. What is the alpha parameter doing? What would you expect to happen for alpha very small? And for alpha very large? Set the option ```normalize=True```. Why is it a good practice to normalize? What effect would the regularization term have over two features, one 10 times larger than the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build two more ridge regression models but with alpha taking values 1e-5 and 10. Then compute the scores over the train and test sets for all models and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, generate a plot with the fitted coefficients for the different models. Use ```plt.plot()``` function and change the ```marker='*','d', 's', 'o'``` and ```color='red', 'blue', 'yellow', 'green'``` parameters to distinguish the different points. Also you should add a legend so the plot is interpretable. Comment on the graphic: How come when alpha is so small you recover the same results as for linear regression? What happens when alpha is bigger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso regression\n",
    "\n",
    "The cost function for Lasso (least absolute shrinkage and selection operator) regression can be written as\n",
    "\n",
    "<img src=\"lasso_formula.png\">\n",
    "\n",
    "This type of regularization (L1) can lead to zero coefficients i.e. some of the features are completely neglected for the evaluation of output. So Lasso regression not only helps in reducing over-fitting but it can help us in feature selection.\n",
    "\n",
    "Generate a Lasso model with ```alpha=1e-1``` and ```normalize=True```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the number of coefficients that are different than 0. To which parameters would you expect they correspond? You may check if you are right by looking at exercise 1 from the previous session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat again the Lasso regression but changing the alpha parameter to 1e-5. How many coefficients will be different than 0 now? And if you use a big alpha, say 100, what should happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally train a linear model, plot the coefficients obtained for the different lasso models and compare them with the linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we will not manually look for the best alpha coefficient. Both in Lasso and ridge regression, the alpha is a hyperparameter. Therefore, we will find the best one via crossvalidation. Fortunately, sklearn has a function that performs CV to find the optimal alpha automatically. Implement a CV with the ```LassoCV``` object. Print the optimal alpha and the score on the test set. Note that there are similar functions for Ridge regression and Elastic net, which has a regularization that combines both L1 and L2 regularizations. Play with the variable `cv` from the `LassoCV` object to perform k-fold cross-validation. What is the minimum value that you can give to `cv`, and the maximum? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
